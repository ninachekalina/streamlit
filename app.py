import os


import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_community.llms import HuggingFaceHub
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
#from langchain_huggingface import HuggingFaceEndpoint
from langchain_community.embeddings import HuggingFaceEmbeddings
import torch


from langchain_community.llms import HuggingFacePipeline

# --- –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ ---
st.set_page_config(page_title="RetrievalQA Chat", layout="centered")
st.title("ü§ñ RetrievalQA –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç")

# --- –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π ---
@st.cache_resource
def load_embeddings():
    return HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')

@st.cache_resource
def load_vector_store(_embeddings):
    return FAISS.load_local('vector_store/tech_big', _embeddings, allow_dangerous_deserialization=True)

@st.cache_resource
def load_llm():
    #local_model_path = r"C:\Users\Nina\.cache\huggingface\hub\models--Qwen--Qwen2.5-1.5B-Instruct\snapshots\989aa7980e4cf806f80c7fef2b1adb7bc71aa306"
    #tokenizer = AutoTokenizer.from_pretrained(local_model_path)
    #model = AutoModelForCausalLM.from_pretrained(local_model_path,
    #device_map="auto",
    #torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)

    model_id = "Qwen/Qwen2.5-1.5B-Instruct"  # –ª–æ–∫–∞–ª—å–Ω–∞—è causal –º–æ–¥–µ–ª—å
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id)
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=600,
        temperature=0.2,
        do_sample=True,
        top_p=0.95,
        top_k=30,
        repetition_penalty=1.1
    )

    return HuggingFacePipeline(pipeline=pipe)
#def load_llm():
    #return HuggingFaceEndpoint(
        #repo_id="Qwen/Qwen3-0.6B",
        #task="text-generation",
        #max_new_tokens=600,
        #temperature=1.5,
        #huggingfacehub_api_token="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"
    #)


    

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ ---
embeddings = load_embeddings()
db = load_vector_store(embeddings)
llm = load_llm()
retriever = db.as_retriever()

# --- Prompt –∏ RetrievalQA chain ---
prompt = PromptTemplate.from_template(
    "–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏. –û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\n"
    "–ï—Å–ª–∏ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å, —Ç–æ —Ç–∞–∫ –∏ –ø–∏—à–∏: –Ø –Ω–µ –∑–Ω–∞—é.\n\n"
    "–í–æ—Ç –ø–æ–ª–µ–∑–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–≤–µ—Ç–∞:\n{context}\n\n"
    "–í–æ–ø—Ä–æ—Å:\n{question}\n\n"
    "–†–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç:"
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type='stuff',
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=True
)

# --- –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ---
user_question = st.text_input("–ó–∞–¥–∞–π—Ç–µ  –≤–æ–ø—Ä–æ—Å :", "–ß—Ç–æ —Ç–∞–∫–æ–µ –±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ?")

if st.button("–ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞–ø—Ä—è–º—É—é"):
    with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞–ø—Ä—è–º—É—é –±–µ–∑ —Ü–µ–ø–æ—á–∫–∏..."):
        response = llm("–ü—Ä–∏–≤–µ—Ç! –†–∞—Å—Å–∫–∞–∂–∏, —á—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç.")
        st.write(response)

if st.button("–ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç") and user_question:
    with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞..."):
        result = qa_chain(user_question)
        st.subheader("–û—Ç–≤–µ—Ç:")
        st.write( result.get('result').split('–†–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç:')[1].strip())

        if result.get("source_documents"):
            st.subheader("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:")
            for i, doc in enumerate(result["source_documents"], 1):
                st.markdown(f"**–ò—Å—Ç–æ—á–Ω–∏–∫ {i}:**\n {doc.page_content[:500]}")
